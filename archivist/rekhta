#!/usr/bin/env python
from bs4 import BeautifulSoup
import re
import urllib2
import json
import requests
from PIL import Image
import sys
import os
import ssl
import uuid

ssl._create_default_https_context = ssl._create_unverified_context

sysArgLen = len(sys.argv)
if not (sysArgLen >= 2 and sysArgLen <= 3):
    print "Download all pages"
    print "\tUsage: python download.py http://bookUrl/"
    print ""
    print "Download specific pages"
    print "\tUsage: python download.py http://bookUrl/ 400-420"
    print ""
    print "Download single page"
    print "\tUsage: python download.py http://bookUrl/ 411"
    sys.exit()

bookUrl = sys.argv[1]

pagesParam = -1
startRange = -1
endRange = -1
if sysArgLen == 3:
    pagesParam = sys.argv[2]

    if "-" in pagesParam:
        rangeArr = pagesParam.split("-")
        if len(rangeArr) == 2 and rangeArr[0] <= rangeArr[1]:
            startRange = int(rangeArr[0]) - 1
            endRange = int(rangeArr[1])
    else:
        pagesParam = int(pagesParam) - 1

print "URL {}".format(bookUrl)
print "Fetching details...."

response = urllib2.urlopen(bookUrl)
html = response.read()

soup = BeautifulSoup(html, "html.parser")
title = soup.h5.string
volume = soup.h6.string
mktemp = uuid.uuid4().hex
os.makedirs(mktemp)
os.chdir(mktemp)


if volume == None:
    name = ("%s") % (title)
else:
    name = ("%s - %s") % (title, volume)

print("Name: "+'\033[92m' + "%s" + '\033[0m') % (name)

scriptText = soup.find(text=re.compile("pages"))

pages = re.findall("[0-9]+.jpg", scriptText)
workingPages = pages
print "Pages: \033[94m{}\n\033[0m".format(len(pages))
#print "Pages: {}\n".format(len(pages))
# for single page

if pagesParam != -1 and "-" not in str(pagesParam):
    workingPages = [pages[pagesParam]]
    print "Processing {}".format(pagesParam + 1)

# for range
if startRange != -1 and endRange != -1:
    workingPages = pages[startRange:endRange]
    print "Processing {}-{}".format(startRange + 1, endRange)

bookIdText = re.findall("bookId.*", scriptText)[0]
bookId = re.findall("[0-9a-zA-Z]{8}-[0-9a-zA-Z]{4}-[0-9a-zA-Z]{4}-[0-9a-zA-Z]{4}-[0-9a-zA-Z]{12}", bookIdText)[0]
pageIds = re.findall("[0-9a-zA-Z]{8}-[0-9a-zA-Z]{4}-[0-9a-zA-Z]{4}-[0-9a-zA-Z]{4}-[0-9a-zA-Z]{12}", scriptText)

for page in workingPages:
    pageIndex = pages.index(page)
    dataUrl = "https://ebooksapi.rekhta.org/api_getebookpagebyid/?pageid=" + pageIds[pageIndex]
    rr = urllib2.urlopen(dataUrl)
    html2 = rr.read()
    data = json.loads(html2)
    print "Downloading {} file".format(page)
    f = open(page, "wb")
    f.write(requests.get("https://ebooksapi.rekhta.org/api_getebookpageimage?ebookid={}&pageid={}".format(bookId, page)).content)
    f.close()
    print "Processing {} file".format(page)
    mainImg = Image.open(page)
    im = Image.new("RGB", (data["PageWidth"],data["PageHeight"]), "white")
    looptimes = data["X"] * data["Y"]
    s = 56
    looptimesRange = range(0, looptimes)
    for x in looptimesRange:
        # data.Sub[i].X1 * (s + 16), data.Sub[i].Y1 * (s + 16), s, s, data.Sub[i].X2 * m, data.Sub[i].Y2 * m, m, m)
        cropX = data["Sub"][x]["X1"] * 66
        cropY = data["Sub"][x]["Y1"] * 66
        pasteX = data["Sub"][x]["X2"] * 50
        pasteY = data["Sub"][x]["Y2"] * 50
        cropBox = (cropX, cropY, cropX + s, cropY + s)
        pasteBox = (pasteX, pasteY)
        cropRegion = mainImg.crop(cropBox)
        im.paste(cropRegion, pasteBox)

    outputFileName = "{}.jpg".format(str(pageIndex + 1).zfill(4))
    im.save(outputFileName, "JPEG", optimize=True)
    if os.path.isfile(page):
        os.remove(page)

    print "Saved output filename {}".format(outputFileName)

    print ""

print("Generting PDF: "+'\033[91m' + "%s.pdf" + '\033[0m') % (name)
os.system('convert *.jpg "%s".pdf' % name)
os.system("mv *.pdf ../")
os.system('rm -fr ../%s' % mktemp)
