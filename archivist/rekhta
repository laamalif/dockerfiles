#!/usr/bin/env python
# -*- coding: utf-8 -*-
from __future__ import unicode_literals
from bs4 import BeautifulSoup
from lxml import html
from lxml.etree import tostring
import re
import urllib2
import json
import requests
from PIL import Image
import sys
import os
import ssl
import uuid

ssl._create_default_https_context = ssl._create_unverified_context

sysArgLen = len(sys.argv)
if not (sysArgLen >= 2 and sysArgLen <= 3):
    print "fetch all pages"
    print "\tUsage: python download.py http://bookUrl/"
    print ""
    print "fetch specific pages"
    print "\tUsage: python download.py http://bookUrl/ 400-420"
    print ""
    print "fetch single page"
    print "\tUsage: python download.py http://bookUrl/ 411"
    sys.exit()

bookUrl = sys.argv[1]

pagesParam = -1
startRange = -1
endRange = -1
if sysArgLen == 3:
    pagesParam = sys.argv[2]

    if "-" in pagesParam:
        rangeArr = pagesParam.split("-")
        if len(rangeArr) == 2 and rangeArr[0] <= rangeArr[1]:
            startRange = int(rangeArr[0]) - 1
            endRange = int(rangeArr[1])
    else:
        pagesParam = int(pagesParam) - 1

print "URL {}".format(bookUrl)
print "fetching details...."

response = urllib2.urlopen(bookUrl)
htmlx = response.read()

soup = BeautifulSoup(htmlx, "html.parser")
title = soup.h5.string
volume = soup.h6.string
mktemp = uuid.uuid4().hex
os.makedirs(mktemp)
os.chdir(mktemp)

h = requests.get(bookUrl).content
tree = html.fromstring(h)

bookData = []

for x in tree.xpath('//div[@class="B-descript"]'):
    title = x.xpath('h5/text()')[0]
    bookD = {'TITLE': title, 'PUBLISHER': '', 'AUTHOR': '', 'YEAR': ''}

    for u in x.xpath('ul/li/p'):
        if u.text.strip() == 'PUBLISHER':
            bookD['PUBLISHER'] = u.xpath('span')[0].text.strip()
        elif u.text.strip() == 'YEAR':
            bookD['YEAR'] = u.xpath('span')[0].text.strip()
        elif u.text.strip() == 'AUTHOR':
            bookD['AUTHOR'] = tostring(u.xpath('span/a')[0], method="text")

    bookData.append(bookD)

author = bookData[0]['AUTHOR']
year = bookData[0]['YEAR']
publisher = bookData[0]['PUBLISHER']

if volume == None:
    name = ("%s") % (title)
else:
    name = ("%s - %s") % (title, volume)

name = re.sub("ØŒ", "-", name)

print("Name: "+'\033[92m' + "%s" + '\033[0m') % (name)
print("Author: "+'\033[93m' + "%s" + '\033[0m') % (author)
print("Year: "+'\033[95m' + "%s" + '\033[0m') % (year)
print("Publisher: "+'\033[94m' + "%s" + '\033[0m') % (publisher)

scriptText = soup.find(text=re.compile("pages"))

pages = re.findall("[0-9]+.jpg", scriptText)
workingPages = pages
print "Pages: \033[94m{}\n\033[0m".format(len(pages))

if pagesParam != -1 and "-" not in str(pagesParam):
    workingPages = [pages[pagesParam]]
    print "Processing {}".format(pagesParam + 1)

# for range
if startRange != -1 and endRange != -1:
    workingPages = pages[startRange:endRange]
    print "Processing {}-{}".format(startRange + 1, endRange)

bookIdText = re.findall("bookId.*", scriptText)[0]
bookId = re.findall("[0-9a-zA-Z]{8}-[0-9a-zA-Z]{4}-[0-9a-zA-Z]{4}-[0-9a-zA-Z]{4}-[0-9a-zA-Z]{12}", bookIdText)[0]
pageIds = re.findall("[0-9a-zA-Z]{8}-[0-9a-zA-Z]{4}-[0-9a-zA-Z]{4}-[0-9a-zA-Z]{4}-[0-9a-zA-Z]{12}", scriptText)

for page in workingPages:
    pageIndex = pages.index(page)
    dataUrl = "https://rbookscdn2.azureedge.net/api_getebookpagebyid/?pageid=" + pageIds[pageIndex]
    rr = urllib2.urlopen(dataUrl)
    html2 = rr.read()
    data = json.loads(html2)
    print "fetching {} file".format(page)
    f = open(page, "wb")
    f.write(requests.get("https://rbookscdn2.azureedge.net/api_getebookpageimage?ebookid={}&pageid={}".format(bookId, page)).content)
    f.close()
    print "processing {} file".format(page)
    mainImg = Image.open(page)
    im = Image.new("RGB", (data["PageWidth"],data["PageHeight"]), "white")
    looptimes = data["X"] * data["Y"]
    s = 56
    looptimesRange = range(0, looptimes)
    for x in looptimesRange:
        # data.Sub[i].X1 * (s + 16), data.Sub[i].Y1 * (s + 16), s, s, data.Sub[i].X2 * m, data.Sub[i].Y2 * m, m, m)
        cropX = data["Sub"][x]["X1"] * 66
        cropY = data["Sub"][x]["Y1"] * 66
        pasteX = data["Sub"][x]["X2"] * 50
        pasteY = data["Sub"][x]["Y2"] * 50
        cropBox = (cropX, cropY, cropX + s, cropY + s)
        pasteBox = (pasteX, pasteY)
        cropRegion = mainImg.crop(cropBox)
        im.paste(cropRegion, pasteBox)

    outputFileName = "{}.jpg".format(str(pageIndex + 1).zfill(4))
    im.save(outputFileName, "JPEG", optimize=True)
    if os.path.isfile(page):
        os.remove(page)

    print "saved output filename {}".format(outputFileName)
    print ""

print("Generting PDF: "+'\033[91m' + "%s.pdf" + '\033[0m') % (name)

if os.path.isfile('../%s.pdf' % name) == True:
    s = name.replace(" ", "_")
    os.system('img2pdf *.jpg --output "%s".pdf' % s)
else:
    os.system('img2pdf *.jpg --output "%s".pdf' % name)

#os.system('exiftool -Title="%s" -Author="%s" -xmp-dc:Publisher="%s" -Copyright="%s" *.pdf' % (title, author, publisher, year))
os.system("mv *.pdf ../")
os.system('rm -fr ../%s' % mktemp)
