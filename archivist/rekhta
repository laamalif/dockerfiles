#!/usr/bin/env python3
# -*- coding: utf-8 -*-
from bs4 import BeautifulSoup
from lxml import html
from lxml.etree import tostring
import re
import urllib.request, urllib.error, urllib.parse
import json
import requests
from PIL import Image
import sys
import os
import ssl
import uuid
import random
ssl._create_default_https_context = ssl._create_unverified_context

sysArgLen = len(sys.argv)
if not (sysArgLen >= 2 and sysArgLen <= 3):
    print("download complete book")
    print("\tUsage: python rekhta.py http://bookUrl/")
    print("")
    print("download page range")
    print("\tUsage: python rekhta.py http://bookUrl/ 400-420")
    print("")
    print("download single page")
    print("\tUsage: python rekhta.py http://bookUrl/ 411")
    sys.exit()

bookUrl = sys.argv[1]

pagesParam = -1
startRange = -1
endRange = -1
if sysArgLen == 3:
    pagesParam = sys.argv[2]

    if "-" in pagesParam:
        rangeArr = pagesParam.split("-")
        if len(rangeArr) == 2 and rangeArr[0] <= rangeArr[1]:
            startRange = int(rangeArr[0]) - 1
            endRange = int(rangeArr[1])
    else:
        pagesParam = int(pagesParam) - 1

print("URL {}".format(bookUrl))
print("Fetching details....")

user_agent = 'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko'
request = urllib.request.Request(bookUrl,headers={'User-Agent': user_agent})
response = urllib.request.urlopen(request)
htmlx = response.read()

user_agents = [
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.13; rv:61.0) Gecko/20100101 Firefox/61.0',
    'Mozilla/5.0 (Wayland; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/11.1.2 Safari/605.1.15',
    'Mozilla/5.0 (iPhone; CPU iPhone OS 11_4_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/11.0 Mobile/15E148 Safari/604.1',
    'Mozilla/5.0 (Linux; Android 8.0.0; SM-G950F Build/R16NW) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 Mobile Safari/537.36',
    'Mozilla/5.0 (Linux; Android 8.1.0; ONEPLUS A5010 Build/OPM1.171019.011) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 Mobile Safari/537.36',
    'Mozilla/5.0 (Linux; Android 8.1.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.107 Mobile Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',
    'Mozilla/5.0 (Linux; Android 9; Pixel 2 XL) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.80 Mobile Safari/537.36',
    'Mozilla/5.0 (Linux; Android 8.1.0) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Focus/6.0 Chrome/67.0.3396.87 Mobile Safari/537.36'
]

soup = BeautifulSoup(htmlx, "html.parser")
title = soup.h5.string
volume = soup.h6.string
mktemp = uuid.uuid4().hex
os.makedirs(mktemp)
os.chdir(mktemp)
user_agent = random.choice(user_agents)
headers = {'User-Agent': user_agent}

h = requests.get(bookUrl, headers=headers).content
tree = html.fromstring(h)

bookData = []

for x in tree.xpath('//div[@class="B-descript"]'):
    title = x.xpath('h5/text()')[0]
    bookD = {'TITLE': title, 'PUBLISHER': '', 'AUTHOR': '', 'YEAR': ''}

    for u in x.xpath('ul/li/p'):
        if u.text.strip() == 'PUBLISHER':
            bookD['PUBLISHER'] = u.xpath('span')[0].text.strip()
        elif u.text.strip() == 'YEAR':
            bookD['YEAR'] = u.xpath('span')[0].text.strip()
        elif u.text.strip() == 'AUTHOR':
            bookD['AUTHOR'] = tostring(u.xpath('span/a')[0], method="text")

    bookData.append(bookD)

author = bookData[0]['AUTHOR']
year = bookData[0]['YEAR']
publisher = bookData[0]['PUBLISHER']


if volume == None:
    name = ("%s") % (title)
else:
    name = ("%s - %s") % (title, volume)

name = re.sub("ØŒ", "-", name)


print(("Name: "+'\033[92m' + "%s" + '\033[0m') % (name))
print(("Author: "+'\033[93m' + "%s" + '\033[0m') % (author))
print(("Year: "+'\033[95m' + "%s" + '\033[0m') % (year))
print(("Publisher: "+'\033[94m' + "%s" + '\033[0m') % (publisher))

scriptText = soup.find(text=re.compile("pages"))

pages = re.findall("[0-9]+.jpg", scriptText)
workingPages = pages
print("Pages: \033[94m{}\n\033[0m".format(len(pages)))
#print "Pages: {}\n".format(len(pages))
# for single page


if pagesParam != -1 and "-" not in str(pagesParam):
    workingPages = [pages[pagesParam]]
    print("Processing {}".format(pagesParam + 1))

# for range
if startRange != -1 and endRange != -1:
    workingPages = pages[startRange:endRange]
    print("Processing {}-{}".format(startRange + 1, endRange))

bookIdText = re.findall("bookId.*", scriptText)[0]
bookId = re.findall("[0-9a-zA-Z]{8}-[0-9a-zA-Z]{4}-[0-9a-zA-Z]{4}-[0-9a-zA-Z]{4}-[0-9a-zA-Z]{12}", bookIdText)[0]
pageIds = re.findall("[0-9a-zA-Z]{8}-[0-9a-zA-Z]{4}-[0-9a-zA-Z]{4}-[0-9a-zA-Z]{4}-[0-9a-zA-Z]{12}", scriptText)

for page in workingPages:
    pageIndex = pages.index(page)
    dataUrl = "https://rbookscdn.azureedge.net/api_getebookpagebyid/?pageid=" + pageIds[pageIndex]
    rr = urllib.request.urlopen(dataUrl)
    html2 = rr.read()
    data = json.loads(html2)
    print("Downloading {} file".format(page))
    f = open(page, "wb")
    f.write(requests.get("https://rbookscdn.azureedge.net/api_getebookpageimage?ebookid={}&pageid={}".format(bookId, page, headers=headers)).content)
    f.close()
    print("Processing {} file".format(page))
    mainImg = Image.open(page)
    im = Image.new("RGB", (data["PageWidth"],data["PageHeight"]), "white")
    looptimes = data["X"] * data["Y"]
    s = 56
    looptimesRange = list(range(0, looptimes))
    for x in looptimesRange:
        # data.Sub[i].X1 * (s + 16), data.Sub[i].Y1 * (s + 16), s, s, data.Sub[i].X2 * m, data.Sub[i].Y2 * m, m, m)
        cropX = data["Sub"][x]["X1"] * 66
        cropY = data["Sub"][x]["Y1"] * 66
        pasteX = data["Sub"][x]["X2"] * 50
        pasteY = data["Sub"][x]["Y2"] * 50
        cropBox = (cropX, cropY, cropX + s, cropY + s)
        pasteBox = (pasteX, pasteY)
        cropRegion = mainImg.crop(cropBox)
        im.paste(cropRegion, pasteBox)

    outputFileName = "{}.jpg".format(str(pageIndex + 1).zfill(4))
    im.save(outputFileName, "JPEG", optimize=True)
    if os.path.isfile(page):
        os.remove(page)

    print("Saved output filename {}".format(outputFileName))

    print("")

print(("Generting PDF: "+'\033[91m' + "%s.pdf" + '\033[0m') % (name))
if os.path.isfile('../%s.pdf' % name) == True:
    s = name.replace(" ", "_")
    os.system('img2pdf *.jpg --output "%s".pdf' % s)
else:
    os.system('img2pdf *.jpg --output "%s".pdf' % name)

os.system('exiftool -Title="%s" -Author="%s" -xmp-dc:Publisher="%s" -Copyright="%s" *.pdf' % (title, author, publisher, year))
os.system("mv *.pdf ../")
os.system('rm -fr ../%s' % mktemp)
