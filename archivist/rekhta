#!/usr/bin/env python3
# -*- coding: utf-8 -*-
from bs4 import BeautifulSoup
from lxml import html
from lxml.etree import tostring
import re
from urllib.request import Request, urlopen
import urllib.error, urllib.parse
#import urllib.request, urllib.error, urllib.parse
import json
import requests
from PIL import Image
import sys
import os
import ssl
import uuid
import random
import time
from requests.adapters import HTTPAdapter

ssl._create_default_https_context = ssl._create_unverified_context

sysArgLen = len(sys.argv)
if not (sysArgLen >= 2 and sysArgLen <= 3):
    print("download complete book")
    print("\tUsage: python rekhta.py http://bookUrl/")
    print("")
    print("download page range")
    print("\tUsage: python rekhta.py http://bookUrl/ 400-420")
    print("")
    print("download single page")
    print("\tUsage: python rekhta.py http://bookUrl/ 411")
    sys.exit()

bookUrl = sys.argv[1]

pagesParam = -1
startRange = -1
endRange = -1
if sysArgLen == 3:
    pagesParam = sys.argv[2]

    if "-" in pagesParam:
        rangeArr = pagesParam.split("-")
        if len(rangeArr) == 2 and rangeArr[0] <= rangeArr[1]:
            startRange = int(rangeArr[0]) - 1
            endRange = int(rangeArr[1])
    else:
        pagesParam = int(pagesParam) - 1

print("URL {}".format(bookUrl))
print("Fetching details....")

user_agent = 'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko'
request = urllib.request.Request(bookUrl,headers={'User-Agent': user_agent})
response = urllib.request.urlopen(request)
htmlx = response.read()

user_agents = [
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Safari/605.1.15',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; rv:96.0) Gecko/20100101 Firefox/96.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',
    'Mozilla/5.0 (iPhone; CPU iPhone OS 15_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/97.0.4692.84 Mobile/15E148 Safari/604.1',
    'Mozilla/5.0 (iPhone; CPU iPhone OS 15_2_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) FxiOS/40.2  Mobile/15E148 Safari/605.1.15',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.98 Safari/537.36 Edg/97.0.4692.98',
    'Mozilla/5.0 (Linux; Android 10; MAR-LX1A) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.98 Mobile Safari/537.36',
    'Mozilla/5.0 (Linux; Android 11; LE2110) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.87 Mobile Safari/537.36'
]

soup = BeautifulSoup(htmlx, "html.parser")
title = soup.h5.string
volume = soup.h6.string
mktemp = uuid.uuid4().hex
os.makedirs(mktemp)
os.chdir(mktemp)
user_agent = random.choice(user_agents)
headers = {'User-Agent': user_agent}

h = requests.get(bookUrl, headers=headers).content
tree = html.fromstring(h)

#bookData = []

#for x in tree.xpath('//div[@class="B-descript"]'):
#    title = x.xpath('h5/text()')[0]
#    bookD = {'TITLE': title, 'PUBLISHER': '', 'AUTHOR': '', 'YEAR': ''}

#    for u in x.xpath('ul/li/p'):
#        if u.text.strip() == 'PUBLISHER':
#            bookD['PUBLISHER'] = u.xpath('span')[0].text.strip()
#        elif u.text.strip() == 'YEAR':
#            bookD['YEAR'] = u.xpath('span')[0].text.strip()
#        elif u.text.strip() == 'AUTHOR':
#            bookD['AUTHOR'] = tostring(u.xpath('span/a')[0], method="text")
#
#    bookData.append(bookD)

#author = bookData[0]['AUTHOR']
#year = bookData[0]['YEAR']
#publisher = bookData[0]['PUBLISHER']


if volume == None:
    name = ("%s") % (title)
else:
    name = ("%s - %s") % (title, volume)

name = re.sub("ØŒ", "-", name)


#print(("Name: "+'\033[92m' + "%s" + '\033[0m') % (name))
#print(("Author: "+'\033[93m' + "%s" + '\033[0m') % (author))
#print(("Year: "+'\033[95m' + "%s" + '\033[0m') % (year))
#print(("Publisher: "+'\033[94m' + "%s" + '\033[0m') % (publisher))

scriptText = soup.find(text=re.compile("pages"))
authtext = soup.find(text=re.compile("authkey"))

pages = re.findall("[0-9]+.jpg", scriptText)
workingPages = pages
print("Pages: \033[94m{}\n\033[0m".format(len(pages)))
#print "Pages: {}\n".format(len(pages))
# for single page


if pagesParam != -1 and "-" not in str(pagesParam):
    workingPages = [pages[pagesParam]]
    print("Processing {}".format(pagesParam + 1))

# for range
if startRange != -1 and endRange != -1:
    workingPages = pages[startRange:endRange]
    print("Processing {}-{}".format(startRange + 1, endRange))

authkeyfull= re.findall("authkey.*cidx", authtext)[0]
authkeyfull = authkeyfull.replace("authkey=", "")
authkeyfull = authkeyfull.replace("&cidx", "")
keyslistA=authkeyfull.split("&bkid=")
authkeyTxt=keyslistA[0]
bkidTxt=keyslistA[1]

bookIdText = re.findall("bookId.*", scriptText)[0]
bookId = re.findall("[0-9a-zA-Z]{8}-[0-9a-zA-Z]{4}-[0-9a-zA-Z]{4}-[0-9a-zA-Z]{4}-[0-9a-zA-Z]{12}", bookIdText)[0]
pageIds = re.findall("[0-9a-zA-Z]{8}-[0-9a-zA-Z]{4}-[0-9a-zA-Z]{4}-[0-9a-zA-Z]{4}-[0-9a-zA-Z]{12}", scriptText)

for page in workingPages:
    pageIndex = pages.index(page)
    #dataUrl = "https://rekhtabookscdn.azureedge.net/api_getebookpagebyid/?pageid=" + pageIds[pageIndex]
    #dataUrl = "https://ebooksapi.rekhta.org/api_getebookpagebyid/?pageid=" + pageIds[pageIndex]
    #dataUrl = "https://ebooksapi.rekhta.org/api_getebookpagebyid/?atky=pns&pgi=" + pageIds[pageIndex]
    dataUrl = "https://ebooksapi.rekhta.org/api_getebookpagebyid/?authkey="+authkeyTxt+"&bkid="+bkidTxt+"&cidx=1&pgid=" + pageIds[pageIndex]
    print(dataUrl)

    req = urllib.request.Request(dataUrl, headers={'Referer': 'https://www.rekhta.org/'})
    html2 = urllib.request.urlopen(req).read()
    print(html2)
    #rr = urllib.request.urlopen(dataUrl)
    #html2 = rr.read()
    data = json.loads(html2)
    print("Downloading {} file".format(page))
    f = open(page, "wb")
    #f.write(requests.get("https://rekhtabookscdn.azureedge.net/api_getebookpageimage?ebookid={}&pageid={}".format(bookId, page, headers=headers)).content)
    #f.write(requests.get("https://alt-ebooksapi.rekhta.org/images/{}/{}".format(bookId, page, headers=headers)).content)
    f.write(requests.get("https://ebooksapi.rekhta.org/images/{}/{}".format(bookId, page, headers=headers)).content)
    f.close()
    print (headers)
    print("Processing {} file".format(page))
    mainImg = Image.open(page)
    im = Image.new("RGB", (data["PageWidth"],data["PageHeight"]), "white")
    looptimes = data["X"] * data["Y"]
    s = 56
    looptimesRange = list(range(0, looptimes))
    for x in looptimesRange:
        # data.Sub[i].X1 * (s + 16), data.Sub[i].Y1 * (s + 16), s, s, data.Sub[i].X2 * m, data.Sub[i].Y2 * m, m, m)
        cropX = data["Sub"][x]["X1"] * 66
        cropY = data["Sub"][x]["Y1"] * 66
        pasteX = data["Sub"][x]["X2"] * 50
        pasteY = data["Sub"][x]["Y2"] * 50
        cropBox = (cropX, cropY, cropX + s, cropY + s)
        pasteBox = (pasteX, pasteY)
        cropRegion = mainImg.crop(cropBox)
        im.paste(cropRegion, pasteBox)

    outputFileName = "{}.jpg".format(str(pageIndex + 1).zfill(4))
    im.save(outputFileName, "JPEG", optimize=True)
    if os.path.isfile(page):
        os.remove(page)

    print("Saved output filename {}".format(outputFileName))

    print("")

print(("Generting PDF: "+'\033[91m' + "%s.pdf" + '\033[0m') % (name))
if os.path.isfile('../%s.pdf' % name) == True:
    s = name.replace(" ", "_")
    os.system('img2pdf *.jpg --output "%s".pdf' % s)
else:
    os.system('img2pdf *.jpg --output "%s".pdf' % name)

#os.system('exiftool -Title="%s" -Author="%s" -xmp-dc:Publisher="%s" -Copyright="%s" *.pdf' % (title, author, publisher, year))
os.system("mv *.pdf ../")
os.system('rm -fr ../%s' % mktemp)
